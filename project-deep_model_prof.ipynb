{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å— 1:\n",
      "The future of \n",
      "sustainability\n",
      "Navigating trends \n",
      "and innovations for \n",
      "a sustainable tomorrow\n",
      "FEBRUARY 2025 | MICHAEL HANF, LEAD SUSTAINABLE BUSINESS, VTT\n",
      "----------------------------------------\n",
      "\n",
      "å— 2:\n",
      "Michael Hanf (2025), The future of sustainability - Navigating \n",
      "trends and innovations for a sustainable tomorrow, \n",
      "VTT Technical Research Centre of Finland, Espoo, Finland.\n",
      "Author: Michael Hanf\n",
      "Contributors: Maria Akerman, Sajad Ashouri, Arash Hajikhani, \n",
      "Kalle Kantola, Tiina Koljonen, Sofi Kurki, Annu Markkula, \n",
      "Maaria Nuutinen, Hanna Pihkola, Antti-Jussi Tahvanainen, \n",
      "Nina Wessberg\n",
      "For enquiries, please contact the author, Michael Hanf, \n",
      "at michael.hanf@vtt.fi\n",
      "Â© VTT Technical Research Centre of Finland, 2025\n",
      "----------------------------------------\n",
      "\n",
      "å— 3:\n",
      "The future of sustainability: \n",
      "Navigating trends and innovations  \n",
      "for a sustainable tomorrow\n",
      "0/ Executive summary  . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "Methodology & approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "The perfect storm  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "Key findings & strategic recommendations . . . . . . . . . . . . . . . . . . . . . 6\n",
      "1/ Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "Purpose and scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "Methodology   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
      "Structure of the report  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2/ The big picture: The future of sustainability . . . 11\n",
      "3/ The perfect storm  . . . . . . . . . . . . . . . . . . . . . . . . 14\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# åŠ è½½PDFæ–‡ä»¶ï¼ˆè¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼‰\n",
    "loader = PyPDFLoader(\"WHITEPAPER_Future_of_Sustainability_2025.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# ä½¿ç”¨æ–‡æœ¬åˆ‡åˆ†å™¨è¿›è¡Œåˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# æå–æ¯ä¸ªå—çš„æ–‡æœ¬å†…å®¹\n",
    "text_lines = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "# æ‰“å°å‰å‡ ä¸ªæ–‡æœ¬å—ä»¥éªŒè¯ç»“æœ\n",
    "for i, text in enumerate(text_lines[:3]):\n",
    "    print(f\"å— {i+1}:\\n{text}\\n{'-'*40}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åµŒå…¥çš„å½¢çŠ¶: (387, 384)\n",
      "ç¬¬ä¸€æ¡åµŒå…¥ç¤ºä¾‹: [ 0.02845221  0.02056801  0.05199016 -0.03178968  0.04272494  0.01241189\n",
      " -0.10353621 -0.0057232  -0.05912128  0.01031158]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒçš„åµŒå…¥æ¨¡å‹\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ä¸ºæ¯ä¸ªæ–‡æœ¬å—ç”ŸæˆåµŒå…¥\n",
    "embeddings = embedding_model.encode(text_lines)\n",
    "\n",
    "# æ£€æŸ¥ç”Ÿæˆçš„åµŒå…¥ç»´åº¦å’Œéƒ¨åˆ†ç»“æœ\n",
    "print(\"åµŒå…¥çš„å½¢çŠ¶:\", np.array(embeddings).shape)\n",
    "print(\"ç¬¬ä¸€æ¡åµŒå…¥ç¤ºä¾‹:\", embeddings[0][:10])  # æ‰“å°ç¬¬ä¸€æ¡åµŒå…¥çš„å‰10ä¸ªæ•°å€¼\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸè¿æ¥åˆ° Milvus!\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections\n",
    "\n",
    "connections.connect(host=\"localhost\", port=\"19530\")\n",
    "print(\"æˆåŠŸè¿æ¥åˆ° Milvus!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "# åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆè¿™é‡Œä¸å†ç”¨ Collectionï¼Œè€Œæ˜¯ MilvusClientï¼‰\n",
    "milvus_client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "\n",
    "collection_name = \"rag_collection\"\n",
    "\n",
    "# åˆ é™¤æ—§é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n",
    "if collection_name in milvus_client.list_collections():\n",
    "    milvus_client.drop_collection(collection_name)\n",
    "    print(f\"âš ï¸ æ—§é›†åˆ {collection_name} å·²åˆ é™¤\")\n",
    "\n",
    "# åˆ›å»ºæ–°é›†åˆ\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=384,  # åµŒå…¥å‘é‡ç»´åº¦\n",
    "    metric_type=\"IP\",  # Inner Product\n",
    "    consistency_level=\"Strong\"\n",
    ")\n",
    "print(f\"âœ… åˆ›å»ºé›†åˆ {collection_name} æˆåŠŸ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ç”¨äºåµŒå…¥çš„å‡½æ•°\n",
    "def emb_text(text):\n",
    "    return embedding_model.encode([text])[0]  # å–ç¬¬ä¸€ä¸ªå‘é‡\n",
    "\n",
    "# æ„é€ æ•°æ®åˆ—è¡¨\n",
    "data = []\n",
    "for i, line in enumerate(text_lines):\n",
    "    data.append({\n",
    "        \"id\": i,\n",
    "        \"vector\": emb_text(line),\n",
    "        \"text\": line\n",
    "    })\n",
    "\n",
    "# æ’å…¥æ•°æ®\n",
    "insert_res = milvus_client.insert(\n",
    "    collection_name=collection_name,\n",
    "    data=data\n",
    ")\n",
    "print(\"âœ… æ’å…¥å®Œæˆï¼Œæ•°é‡ï¼š\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, top_k=10):\n",
    "    query_vector = emb_text(query)\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[query_vector],\n",
    "        limit=top_k,\n",
    "        search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "\n",
    "    results = [hit[\"entity\"][\"text\"] for hit in search_res[0]]\n",
    "    return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "Use the information enclosed in <context> tags to answer the user's question.\n",
    "Only use factual data found in the context â€” do not make up anything.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Answer with a clear list of bullet points if appropriate. Respond \"Not found in the provided context\" if the context doesn't contain the answer.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_prompt(context, question):\n",
    "    return PROMPT.format(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt, tokenizer, model):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def answer_question(question):\n",
    "    context = retrieve_context(question)\n",
    "    prompt = build_prompt(context, question)\n",
    "    answer = generate_answer(prompt, tokenizer, model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the 2030 emission reduction target mentioned in the decarbonisation section?\"\n",
    "context = retrieve_context(question)\n",
    "\n",
    "print(\"ğŸ“š Context:\\n\", context)\n",
    "prompt = build_prompt(context, question)\n",
    "print(\"ğŸ§¾ Prompt:\\n\", prompt)\n",
    "\n",
    "answer = generate_answer(prompt, tokenizer, model)\n",
    "print(\"ğŸ¤– Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
