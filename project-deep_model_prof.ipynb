{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 1:\n",
      "The future of \n",
      "sustainability\n",
      "Navigating trends \n",
      "and innovations for \n",
      "a sustainable tomorrow\n",
      "FEBRUARY 2025 | MICHAEL HANF, LEAD SUSTAINABLE BUSINESS, VTT\n",
      "----------------------------------------\n",
      "\n",
      "块 2:\n",
      "Michael Hanf (2025), The future of sustainability - Navigating \n",
      "trends and innovations for a sustainable tomorrow, \n",
      "VTT Technical Research Centre of Finland, Espoo, Finland.\n",
      "Author: Michael Hanf\n",
      "Contributors: Maria Akerman, Sajad Ashouri, Arash Hajikhani, \n",
      "Kalle Kantola, Tiina Koljonen, Sofi Kurki, Annu Markkula, \n",
      "Maaria Nuutinen, Hanna Pihkola, Antti-Jussi Tahvanainen, \n",
      "Nina Wessberg\n",
      "For enquiries, please contact the author, Michael Hanf, \n",
      "at michael.hanf@vtt.fi\n",
      "© VTT Technical Research Centre of Finland, 2025\n",
      "----------------------------------------\n",
      "\n",
      "块 3:\n",
      "The future of sustainability: \n",
      "Navigating trends and innovations  \n",
      "for a sustainable tomorrow\n",
      "0/ Executive summary  . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "Methodology & approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "The perfect storm  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "Key findings & strategic recommendations . . . . . . . . . . . . . . . . . . . . . 6\n",
      "1/ Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "Purpose and scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "Methodology   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
      "Structure of the report  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2/ The big picture: The future of sustainability . . . 11\n",
      "3/ The perfect storm  . . . . . . . . . . . . . . . . . . . . . . . . 14\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 加载PDF文件（请确保文件路径正确）\n",
    "loader = PyPDFLoader(\"WHITEPAPER_Future_of_Sustainability_2025.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 使用文本切分器进行切分\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# 提取每个块的文本内容\n",
    "text_lines = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "# 打印前几个文本块以验证结果\n",
    "for i, text in enumerate(text_lines[:3]):\n",
    "    print(f\"块 {i+1}:\\n{text}\\n{'-'*40}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入的形状: (387, 384)\n",
      "第一条嵌入示例: [ 0.02845221  0.02056801  0.05199016 -0.03178968  0.04272494  0.01241189\n",
      " -0.10353621 -0.0057232  -0.05912128  0.01031158]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 加载预训练的嵌入模型\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 为每个文本块生成嵌入\n",
    "embeddings = embedding_model.encode(text_lines)\n",
    "\n",
    "# 检查生成的嵌入维度和部分结果\n",
    "print(\"嵌入的形状:\", np.array(embeddings).shape)\n",
    "print(\"第一条嵌入示例:\", embeddings[0][:10])  # 打印第一条嵌入的前10个数值\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功连接到 Milvus!\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections\n",
    "\n",
    "connections.connect(host=\"localhost\", port=\"19530\")\n",
    "print(\"成功连接到 Milvus!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "# 初始化客户端（这里不再用 Collection，而是 MilvusClient）\n",
    "milvus_client = MilvusClient(uri=\"http://localhost:19530\")\n",
    "\n",
    "collection_name = \"rag_collection\"\n",
    "\n",
    "# 删除旧集合（如果存在）\n",
    "if collection_name in milvus_client.list_collections():\n",
    "    milvus_client.drop_collection(collection_name)\n",
    "    print(f\"⚠️ 旧集合 {collection_name} 已删除\")\n",
    "\n",
    "# 创建新集合\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=384,  # 嵌入向量维度\n",
    "    metric_type=\"IP\",  # Inner Product\n",
    "    consistency_level=\"Strong\"\n",
    ")\n",
    "print(f\"✅ 创建集合 {collection_name} 成功\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义用于嵌入的函数\n",
    "def emb_text(text):\n",
    "    return embedding_model.encode([text])[0]  # 取第一个向量\n",
    "\n",
    "# 构造数据列表\n",
    "data = []\n",
    "for i, line in enumerate(text_lines):\n",
    "    data.append({\n",
    "        \"id\": i,\n",
    "        \"vector\": emb_text(line),\n",
    "        \"text\": line\n",
    "    })\n",
    "\n",
    "# 插入数据\n",
    "insert_res = milvus_client.insert(\n",
    "    collection_name=collection_name,\n",
    "    data=data\n",
    ")\n",
    "print(\"✅ 插入完成，数量：\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, top_k=10):\n",
    "    query_vector = emb_text(query)\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[query_vector],\n",
    "        limit=top_k,\n",
    "        search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "\n",
    "    results = [hit[\"entity\"][\"text\"] for hit in search_res[0]]\n",
    "    return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "Use the information enclosed in <context> tags to answer the user's question.\n",
    "Only use factual data found in the context — do not make up anything.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Answer with a clear list of bullet points if appropriate. Respond \"Not found in the provided context\" if the context doesn't contain the answer.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_prompt(context, question):\n",
    "    return PROMPT.format(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt, tokenizer, model):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def answer_question(question):\n",
    "    context = retrieve_context(question)\n",
    "    prompt = build_prompt(context, question)\n",
    "    answer = generate_answer(prompt, tokenizer, model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the 2030 emission reduction target mentioned in the decarbonisation section?\"\n",
    "context = retrieve_context(question)\n",
    "\n",
    "print(\"📚 Context:\\n\", context)\n",
    "prompt = build_prompt(context, question)\n",
    "print(\"🧾 Prompt:\\n\", prompt)\n",
    "\n",
    "answer = generate_answer(prompt, tokenizer, model)\n",
    "print(\"🤖 Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
